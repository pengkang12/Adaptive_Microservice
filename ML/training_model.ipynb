{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (from scipy) (1.19.5)\n",
      "\n",
      "PackageNotInstalledError: Package is not installed in prefix.\n",
      "  prefix: /home/peng/anaconda2\n",
      "  package name: scikit-learn\n",
      "\n",
      "\n",
      "Requirement already satisfied: scikit-learn in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/peng/Desktop/2019-work/env/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!conda update scikit-learn\n",
    "\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_predict, RandomizedSearchCV, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic, WhiteKernel, Matern\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "import joblib\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import sqrt\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def adj_r2_score(p,y,yhat):\n",
    "        \"\"\"Adjusted R square â€” put fitted linear model, y value, estimated y value in order\"\"\"\n",
    "        adj = 1 - float(len(y)-1)/(len(y)-p-1)*(1 - r2_score(y,yhat))\n",
    "        return adj\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "class predictY(Enum):\n",
    "    CATALOGUE = 1\n",
    "    CART = 2\n",
    "    WEB = 3\n",
    "    SHIPPING = 4\n",
    "\n",
    "class X(Enum):\n",
    "    PODCPU = 1\n",
    "    PODCPU_VMCPU = 2\n",
    "    PODCPU_VMCPI = 3\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'figure.figsize': (4, 3),\n",
    "         #'font.family': 'serif',\n",
    "         'font.sans-serif': 'Arial', \n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         #'axes.labelweight':'bold', \n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "filter_metrics = [\n",
    "        'cart_cont_cpu5s_avg','web_cont_cpu5s_avg','catalogue_cont_cpu5s_avg', 'ratings_cont_cpu5s_avg',\n",
    "        'mysql_cont_cpu5s_avg', 'mongodb_cont_cpu5s_avg', 'dispatch_cont_cpu5s_avg', 'payment_cont_cpu5s_avg',\n",
    "        'rabbitmq_cont_cpu5s_avg', 'user_cont_cpu5s_avg','shipping_cont_cpu5s_avg','redis_cont_cpu5s_avg',\n",
    "\n",
    "        'catalogue_vm_netW', 'ratings_vm_netW', 'cart_vm_netW', 'web_vm_netW',\n",
    "        'mysql_vm_netW','mongodb_vm_netW', 'dispatch_vm_netW', 'payment_vm_netW',\n",
    "        'rabbitmq_vm_netW','shipping_vm_netW','redis_vm_netW', 'user_vm_netW',\n",
    "\n",
    "        'catalogue_vm_netR', 'ratings_vm_netR', 'cart_vm_netR', 'web_vm_netR',\n",
    "        'mysql_vm_netR','mongodb_vm_netR', 'dispatch_vm_netR', 'payment_vm_netR',\n",
    "        'rabbitmq_vm_netR','shipping_vm_netR','redis_vm_netR', 'user_vm_netR',\n",
    "\n",
    "        'mysql_vm_util', 'ratings_vm_util','cart_vm_util','catalogue_vm_util',\n",
    "        'web_vm_util','mongodb_vm_util', 'dispatch_vm_util', 'payment_vm_util',\n",
    "        'rabbitmq_vm_util','shipping_vm_util', 'redis_vm_util', 'user_vm_util',\n",
    "\n",
    "        'user_perf_cpi', 'redis_perf_cpi','mysql_perf_cpi', 'ratings_perf_cpi',\n",
    "        'cart_perf_cpi','catalogue_perf_cpi','web_perf_cpi','mongodb_perf_cpi',\n",
    "        'dispatch_perf_cpi', 'payment_perf_cpi','rabbitmq_perf_cpi','shipping_perf_cpi',\n",
    "\n",
    "\n",
    "        'redis_perf_llc', 'user_perf_llc','mysql_perf_llc', 'ratings_perf_llc',\n",
    "        'cart_perf_llc','catalogue_perf_llc','web_perf_llc','mongodb_perf_llc',\n",
    "        'dispatch_perf_llc', 'payment_perf_llc','rabbitmq_perf_llc','shipping_perf_llc',\n",
    "\n",
    "        'cart_cont_netW5s_avg', 'web_cont_netW5s_avg','ratings_cont_netW5s_avg','mysql_cont_netW5s_avg',\n",
    "        'catalogue_cont_netW5s_avg','mongodb_cont_netW5s_avg', 'dispatch_cont_netW5s_avg', 'payment_cont_netW5s_avg',\n",
    "        'rabbitmq_cont_netW5s_avg','shipping_cont_netW5s_avg','redis_cont_netW5s_avg', 'user_cont_netW5s_avg',\n",
    "    \n",
    "        'cart_cont_netR5s_avg', 'web_cont_netR5s_avg','ratings_cont_netR5s_avg','mysql_cont_netR5s_avg',\n",
    "        'catalogue_cont_netR5s_avg','mongodb_cont_netR5s_avg', 'dispatch_cont_netR5s_avg', 'payment_cont_netR5s_avg',\n",
    "        'rabbitmq_cont_netR5s_avg','shipping_cont_netR5s_avg','redis_cont_netR5s_avg', 'user_cont_netR5s_avg',\n",
    "    \n",
    "        'cart_cont_memR5s_avg', 'web_cont_memR5s_avg','ratings_cont_memR5s_avg','mysql_cont_memR5s_avg',\n",
    "        'catalogue_cont_memR5s_avg','mongodb_cont_memR5s_avg', 'dispatch_cont_memR5s_avg', 'payment_cont_memR5s_avg',\n",
    "        'rabbitmq_cont_memR5s_avg','shipping_cont_memR5s_avg','redis_cont_memR5s_avg', 'user_cont_memR5s_avg',\n",
    "    \n",
    "            'cart_cont_memW5s_avg', 'web_cont_memW5s_avg','ratings_cont_memW5s_avg','mysql_cont_memW5s_avg',\n",
    "        'catalogue_cont_memW5s_avg','mongodb_cont_memW5s_avg', 'dispatch_cont_memW5s_avg', 'payment_cont_memW5s_avg',\n",
    "        'rabbitmq_cont_memW5s_avg','shipping_cont_memW5s_avg','redis_cont_memW5s_avg', 'user_cont_memW5s_avg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes=(5,3, 3)\n",
    "\n",
    "def training_by_different_model(dfnorm, y):\n",
    "    svr = svm.SVR(kernel='linear')\n",
    "    lr = LinearRegression()\n",
    "    dt = DecisionTreeRegressor()\n",
    "    rf = RandomForestRegressor()\n",
    "    #kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) + WhiteKernel(noise_level=0.5)\n",
    "    #kernel = 0.5**2 * RationalQuadratic(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "    #kernel = 0.75**2 * RationalQuadratic(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "    kernel =  50.0**2 * RBF(length_scale=50.0) + 0.5**2 * RationalQuadratic(length_scale=1.0)# + WhiteKernel(noise_level=0.1)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel,n_restarts_optimizer=10,normalize_y=True)\n",
    "\n",
    "    nn = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=hidden_layer_sizes, random_state=1, max_iter=1000, activation='relu',learning_rate_init='0.01',momentum=0.9)\n",
    "\n",
    "    predicted_lr = cross_val_predict(lr, dfnorm, y, cv=10)\n",
    "    predicted_svr = cross_val_predict(svr, dfnorm, y, cv=10)\n",
    "    predicted_dt = cross_val_predict(dt, dfnorm, y, cv=10)\n",
    "    predicted_rf = cross_val_predict(rf, dfnorm, y, cv=10)\n",
    "    predicted_gp = cross_val_predict(gp, dfnorm, y, cv=10)\n",
    "    predicted_nn = cross_val_predict(nn, dfnorm, y, cv=10)\n",
    "\n",
    "    # do not run until the previous step finishes execution. gaussian process and neural network cross validation takes some time.\n",
    "\n",
    "    print(\"\\tLR\\tSVR\\tDT\\tRF\\tNN\\tGP\")\n",
    "    cv_lr = mean_absolute_error(y, predicted_lr)\n",
    "    cv_svr = mean_absolute_error(y, predicted_svr)\n",
    "    cv_dt = mean_absolute_error(y, predicted_dt)\n",
    "    cv_rf = mean_absolute_error(y, predicted_rf)\n",
    "    cv_nn = mean_absolute_error(y, predicted_nn)\n",
    "    cv_gp = mean_absolute_error(y, predicted_gp)\n",
    "    cv_gp = 0\n",
    "    \n",
    "    print(\"mae\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\" % (cv_lr, cv_svr, cv_dt, cv_rf, cv_nn, cv_gp))\n",
    "    cv_lr = mean_absolute_percentage_error(y, predicted_lr)\n",
    "    cv_svr = mean_absolute_percentage_error(y, predicted_svr)\n",
    "    cv_dt = mean_absolute_percentage_error(y, predicted_dt)\n",
    "    cv_rf = mean_absolute_percentage_error(y, predicted_rf)\n",
    "    cv_nn = mean_absolute_percentage_error(y, predicted_nn)\n",
    "    cv_gp = mean_absolute_percentage_error(y, predicted_gp)\n",
    "    ##cv_gp = 0\n",
    "\n",
    "    print(\"mape\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\" % (cv_lr, cv_svr, cv_dt, cv_rf, cv_nn, cv_gp))\n",
    "\n",
    "    cv_lr = sqrt(mean_squared_error(y, predicted_lr))\n",
    "    cv_svr = sqrt(mean_squared_error(y, predicted_svr))\n",
    "    cv_dt = sqrt(mean_squared_error(y, predicted_dt))\n",
    "    cv_rf = sqrt(mean_squared_error(y, predicted_rf))\n",
    "    cv_nn = sqrt(mean_squared_error(y, predicted_nn))\n",
    "    cv_gp = sqrt(mean_squared_error(y, predicted_gp))\n",
    "    #cv_gp = 0\n",
    "\n",
    "    print(\"rmse\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\" % (cv_lr, cv_svr, cv_dt, cv_rf, cv_nn, cv_gp))\n",
    "\n",
    "\n",
    "    print(\"r2\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\" % (r2_score(y, predicted_lr), r2_score(y, predicted_svr), r2_score(y, predicted_dt), r2_score(y, predicted_rf), r2_score(y, predicted_nn), r2_score(y, predicted_gp)))\n",
    "    #print(\"r2\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\" % (adj_r2_score(p, y, predicted_lr), r2_score(y, predicted_svr), r2_score(y, predicted_dt), r2_score(y, predicted_rf), r2_score(y, predicted_nn)))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    #plt.title('Cross-validated predictions of 95th percentile latency (ms)')\n",
    "    ax.scatter(y, predicted_gp, edgecolors=(0, 0, 0))\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured tail latency (ms)')\n",
    "    ax.set_ylabel('Predicted tail latency (ms)')\n",
    "    #ax.set_xlim(50,500)\n",
    "    #ax.set_ylim(50,500)\n",
    "    plt.grid(True)\n",
    "    #plt.xticks(np.arange(0, 501, step=100))\n",
    "    #plt.yticks(np.arange(0, 501, step=100))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return \n",
    "    fig, ax = plt.subplots()\n",
    "    #plt.title('Cross-validated predictions of 95th percentile latency (ms)')\n",
    "    ax.scatter(y, predicted_nn, edgecolors=(0, 0, 0))\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured tail latency (ms)')\n",
    "    ax.set_ylabel('Predicted tail latency (ms)')\n",
    "    #ax.set_xlim(50,500)\n",
    "    #ax.set_ylim(50,500)\n",
    "    plt.grid(True)\n",
    "    #plt.xticks(np.arange(0, 501, step=100))\n",
    "    #plt.yticks(np.arange(0, 501, step=100))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "def feature_selection(rdf, y1):\n",
    "    #feature importance\n",
    "    #rlasso = RandomizedLasso(alpha=0.025, selection_threshold=0.2)\n",
    "    rlasso = RandomizedLasso(selection_threshold=0.2)\n",
    "\n",
    "    dfnew = rlasso.fit_transform(rdf, y1)\n",
    "    print(\"Features sorted by their score:\")\n",
    "    print(\"rlasso \", rlasso.scores_)\n",
    "    #print \"rf \",rf.feature_importances_\n",
    "    #print(rdf.columns)\n",
    "    ret = []\n",
    "    column = rdf.columns.tolist()\n",
    "    for i in range(len(rlasso.scores_)):\n",
    "        if rlasso.scores_[i] >= 0.01:\n",
    "            ret.append(column[i])\n",
    "    print(\"feature selection length is \", len(ret))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'test_id', u'cart_vm_util', u'cart_perf_cpi', u'cart_perf_llc',\n",
      "       u'cart_vm_netW', u'cart_vm_netR', u'cart_95th_latency',\n",
      "       u'cart_cont_cpu5s_avg', u'cart_cont_memW5s_avg',\n",
      "       u'cart_cont_memR5s_avg',\n",
      "       ...\n",
      "       u'web_perf_cpi', u'web_perf_llc', u'web_vm_netW', u'web_vm_netR',\n",
      "       u'web_95th_latency', u'web_cont_cpu5s_avg', u'web_cont_memW5s_avg',\n",
      "       u'web_cont_memR5s_avg', u'web_cont_netW5s_avg', u'web_cont_netR5s_avg'],\n",
      "      dtype='object', length=133)\n",
      "(696, 133)\n",
      "(696, 120)\n",
      "Features sorted by their score:\n",
      "('rlasso ', array([0.135, 0.   , 0.975, 0.095, 0.005, 0.495, 0.   , 0.   , 0.125,\n",
      "       0.005, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.68 , 0.   , 0.73 , 0.035, 0.78 , 0.   , 0.   , 0.005,\n",
      "       0.075, 0.   , 0.005, 0.22 , 0.   , 0.07 , 0.195, 0.   , 0.785,\n",
      "       0.025, 0.01 , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.02 ,\n",
      "       0.16 , 0.04 , 0.535, 0.   , 0.285, 0.   , 0.08 , 0.   , 0.   ,\n",
      "       0.165, 0.   , 0.055, 0.055, 0.44 , 0.235, 0.04 , 0.   , 0.15 ,\n",
      "       0.   , 0.01 , 0.155, 0.14 , 0.   , 0.085, 0.005, 0.22 , 0.135,\n",
      "       0.23 , 0.   , 0.   , 0.   , 0.   , 0.065, 0.   , 0.   , 0.16 ,\n",
      "       0.   , 1.   , 0.585, 0.   , 0.   , 0.   , 0.   , 0.   , 0.255,\n",
      "       0.   , 0.   , 0.   , 0.   , 0.   , 0.305, 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.   , 0.   ]))\n",
      "('feature selection length is ', 42)\n"
     ]
    }
   ],
   "source": [
    "rdf_all = pd.read_csv('../training_data/bigtable.csv')\n",
    "print(rdf_all.columns)\n",
    "\n",
    "hidden_layer_sizes=(5,3, 3)\n",
    "def rdf_filter(rdf):\n",
    "    \n",
    "    # outlier removal\n",
    "    rdf = rdf[rdf.test_id.str.contains('.*_[0-6]_*')]\n",
    "\n",
    "    new = rdf[\"test_id\"].str.split(\"_\", n = 4, expand = True)\n",
    "    rdf = rdf.drop(columns=['test_id'])\n",
    "    rdf['thread'] = new[2]\n",
    "    rdf = rdf.fillna(0)\n",
    "    rdf = rdf.dropna()\n",
    "    #subset=filter_metrics)\n",
    "    return rdf\n",
    "\n",
    "rdf_all = rdf_filter(rdf_all)\n",
    "\n",
    "caseY = predictY.CATALOGUE\n",
    "#caseY = predictY.WEB\n",
    "#caseY = predictY.CART\n",
    "#caseY = predictY.SHIPPING\n",
    "\n",
    "#y  = rdf.catalogue_95th_latency\n",
    "\n",
    "predict_list_y = [\"catalogue_95th_latency\", \"cart_95th_latency\",\n",
    "                  \"shipping_95th_latency\", \"payment_95th_latency\", \"ratings_95th_latency\"]\n",
    "\n",
    "\n",
    "print(rdf_all.shape)\n",
    "\n",
    "for workflow in predict_list_y:\n",
    "    #print(\"training \", locals()[y])\n",
    "    rdf = rdf_all[filter_metrics]\n",
    "    print(rdf.shape)\n",
    "    conf_filter=feature_selection(rdf, rdf_all[[workflow]])\n",
    "    hidden_layer_sizes=(5,3, 3)\n",
    "    rdf = rdf[np.abs(rdf_all[workflow] - rdf_all[workflow].mean()) <= (3*rdf_all[workflow].std())]\n",
    "\n",
    "    df = rdf[conf_filter]\n",
    "    y = rdf_all[[workflow]][np.abs(rdf_all[workflow] - rdf_all[workflow].mean()) <= (3*rdf_all[workflow].std())]\n",
    "    scaler = StandardScaler()\n",
    "    dfnorm = scaler.fit_transform(df)\n",
    "\n",
    "    training_by_different_model(dfnorm, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'models/neuralnet.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-273165c8d301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/neuralnet.sav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/gp.sav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models.scaler.sav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/neuralnet.sav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peng/anaconda2/lib/python2.7/site-packages/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'models/neuralnet.sav'"
     ]
    }
   ],
   "source": [
    "joblib.dump(nn, 'models/neuralnet.sav')\n",
    "joblib.dump(gp, 'models/gp.sav')\n",
    "joblib.dump(scaler, 'models.scaler.sav')\n",
    "loaded_model = joblib.load('models/neuralnet.sav')\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(dfnorm, y, random_state=42)  \n",
    "nn.fit(train_X, train_y)\n",
    "gp.fit(train_X, train_y)\n",
    "\n",
    "result = loaded_model.score(test_X, test_y)\n",
    "print (\"done\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
